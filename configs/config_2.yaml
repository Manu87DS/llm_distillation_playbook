model_type: llm
input_features:
  - name: prompt
    type: text
    preprocessing:
      max_sequence_length: null
    column: prompt
output_features:
  - name: is_bad
    type: text
    preprocessing:
      max_sequence_length: null
    column: is_bad
prompt:
  template: >-
    Given the input text below, classify whether it is toxic. If the input text
    is toxic, return 'true', otherwise return 'false'.

    Input text: {comment_text}

    Output:
preprocessing:
  global_max_sequence_length: 2048
adapter:
  type: lora
generation:
  max_new_tokens: 3
trainer:
  type: finetune
  epochs: 10
  batch_size: 1
  learning_rate: 0.0002
  eval_batch_size: 12
  learning_rate_scheduler:
    decay: cosine
    warmup_fraction: 0.03
  gradient_accumulation_steps: 16
  enable_gradient_checkpointing: true
base_model: meta-llama/Llama-2-7b-chat-hf
ludwig_version: 0.9.dev
